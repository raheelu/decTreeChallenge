---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
format:
  html: default
  pdf: default
execute:
  echo: true
  eval: true
---

# üå≥ Decision Tree Challenge - Feature Importance and Variable Encoding


## The Decision Tree Problem üéØ

> "The most important thing in communication is hearing what isn't said." - Peter Drucker

**The Core Problem:** Decision trees are often praised for their interpretability and ability to handle both numerical and categorical variables. But what happens when we encode categorical variables as numbers? How does this affect our understanding of feature importance?

**What is Feature Importance?** In decision trees, feature importance measures how much each variable contributes to reducing impurity (or improving prediction accuracy) across all splits in the tree. It's a key metric for understanding which variables matter most for your predictions.

::: {.callout-important}
## üéØ The Key Insight: Encoding Matters for Interpretability

**The problem:** When we encode categorical variables as numerical values (like 1, 2, 3, 4...), decision trees treat them as if they have a meaningful numerical order. This can completely distort our analysis.

**The Real-World Context:** In real estate, we know that neighborhood quality, house style, and other categorical factors are crucial for predicting home prices. But if we encode these as numbers, we might get misleading insights about which features actually matter most.

**The Devastating Reality:** Even sophisticated machine learning models can give us completely wrong insights about feature importance if we don't properly encode our variables. A categorical variable that should be among the most important might appear irrelevant, while a numerical variable might appear artificially important.

:::

Let's assume we want to predict house prices and understand which features matter most. The key question is: **How does encoding categorical variables as numbers affect our understanding of feature importance?**

## The Ames Housing Dataset üè†

We are analyzing the Ames Housing dataset which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is perfect for our analysis because it contains a categorical variable (like zip code) and numerical variables (like square footage, year built, number of bedrooms).

## The Problem: ZipCode as Numerical vs Categorical

**Key Question:** What happens when we treat zipCode as a numerical variable in a decision tree? How does this affect feature importance interpretation?

**The Issue:** Zip codes (50010, 50011, 50012, 50013) are categorical variables representing discrete geographic areas, i.e. neighborhoods. When treated as numerical, the tree might split on "zipCode > 50012.5" - which has no meaningful interpretation for house prices.  Zip codes are non-ordinal categorical variables meaning they have no inherent order that aids house price prediction (i.e. zip code 99999 is not the priceiest zip code).

## Data Loading and Model Building

### Python

```{python}
#| label: load-and-model-python
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Load data
sales_data = pd.read_csv("salesPriceData.csv")

# Prepare model data (treating zipCode as numerical)
model_vars = ['SalePrice', 'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath', 
              'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode']
model_data = sales_data[model_vars].dropna()

# Split data
X = model_data.drop('SalePrice', axis=1)
y = model_data['SalePrice']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# Build decision tree
tree_model = DecisionTreeRegressor(max_depth=3, 
                                  min_samples_split=20, 
                                  min_samples_leaf=10, 
                                  random_state=123)
tree_model.fit(X_train, y_train)

print(f"Model built with {tree_model.get_n_leaves()} terminal nodes")
```

## Tree Visualization

### Python

```{python}
#| label: visualize-tree-python
#| echo: false
#| fig-width: 10
#| fig-height: 6

# Visualize tree
plt.figure(figsize=(10, 6))
plot_tree(tree_model, 
          feature_names=X_train.columns,
          filled=True, 
          rounded=True,
          fontsize=10,
          max_depth=3)
plt.title("Decision Tree (zipCode as Numerical)")
plt.tight_layout()
plt.show()
```

:::

## Feature Importance Analysis

### Python

```{python}
#| label: feature-importance-python
#| echo: false

# Extract and display feature importance
importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': tree_model.feature_importances_
}).sort_values('Importance', ascending=False)

importance_df['Importance_Percent'] = (importance_df['Importance'] * 100).round(2)

# Check zipCode ranking
zipcode_rank = importance_df[importance_df['Feature'] == 'zipCode'].index[0] + 1
zipcode_importance = importance_df[importance_df['Feature'] == 'zipCode']['Importance_Percent'].iloc[0]
```

```{python}
#| label: importance-plot-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance
plt.figure(figsize=(8, 5))
plt.barh(range(len(importance_df)), importance_df['Importance'], 
         color='steelblue', alpha=0.7)
plt.yticks(range(len(importance_df)), importance_df['Feature'])
plt.xlabel('Importance Score')
plt.title('Feature Importance (zipCode as Numerical)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```

## Critical Analysis: The Encoding Problem

::: {.callout-warning}
## ‚ö†Ô∏è The Problem Revealed

**What to note:** Our decision tree treated `zipCode` as a numerical variable.  This leads to zip code being unimportant.  Not surprisingly, because there is no reason to believe allowing splits like "zipCode < 50012.5" should be beneficial for house price prediction. This false coding of a variable creates several problems:

1. **Potentially Meaningless Splits:** A zip code of 50013 is not "greater than" 50012 in any meaningful way for house prices
2. **False Importance:** The algorithm assigns importance to zipCode based on numerical splits rather than categorical distinctions OR the importance of zip code is completely missed as numerical ordering has no inherent relationship to house prices.
3. **Misleading Interpretations:** We might conclude zipCode is not important when our intuition tells us it should be important (listen to your intuition).

**The Real Issue:** Zip codes are categorical variables representing discrete geographic areas. The numerical values have no inherent order or magnitude relationship to house prices.  These must be modelled as categorical variables.
:::

## Proper Categorical Encoding: The Solution

Now let's repeat the analysis with zipCode properly encoded as categorical variables to see the difference.

**Python Approach:** One-hot encode zipCode (create dummy variables for each zip code)

### Categorical Encoding Analysis

### Python

```{python}
#| label: categorical-python
#| echo: false
#| include: false
# One-hot encode zipCode
import pandas as pd

# Create one-hot encoded zipCode
zipcode_encoded = pd.get_dummies(model_data['zipCode'], prefix='zipCode')
model_data_cat = pd.concat([model_data.drop('zipCode', axis=1), zipcode_encoded], axis=1)

# Split data
X_cat = model_data_cat.drop('SalePrice', axis=1)
y_cat = model_data_cat['SalePrice']
X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y_cat, test_size=0.2, random_state=123)

# Build decision tree with one-hot encoded zipCode
tree_model_cat = DecisionTreeRegressor(max_depth=3, 
                                      min_samples_split=20, 
                                      min_samples_leaf=10, 
                                      random_state=123)
tree_model_cat.fit(X_train_cat, y_train_cat)

# Feature importance with one-hot encoded zipCode
importance_cat_df = pd.DataFrame({
    'Feature': X_train_cat.columns,
    'Importance': tree_model_cat.feature_importances_
}).sort_values('Importance', ascending=False)

importance_cat_df['Importance_Percent'] = (importance_cat_df['Importance'] * 100).round(2)

# Check zipCode features
zipcode_features = [col for col in X_train_cat.columns if col.startswith('zipCode')]
zipcode_importance = importance_cat_df[importance_cat_df['Feature'].isin(zipcode_features)]['Importance'].sum()
total_importance = importance_cat_df['Importance'].sum()
zipcode_percent = (zipcode_importance / total_importance * 100).round(2)
```

### Tree Visualization: Categorical zipCode

### Python

```{python}
#| label: visualize-tree-cat-python
#| echo: false
#| fig-width: 10
#| fig-height: 6

# Visualize tree with one-hot encoded zipCode
plt.figure(figsize=(10, 6))
plot_tree(tree_model_cat, 
          feature_names=X_train_cat.columns,
          filled=True, 
          rounded=True,
          fontsize=8,
          max_depth=4)
plt.title("Decision Tree (zipCode One-Hot Encoded)")
plt.tight_layout()
plt.show()
```

### Feature Importance: Categorical zipCode

### Python
```{python}
#| label: importance-plot-cat-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance for categorical zipCode
plt.figure(figsize=(8, 5))
plt.barh(range(len(importance_cat_df)), importance_cat_df['Importance'], 
         color='darkgreen', alpha=0.7)
plt.yticks(range(len(importance_cat_df)), importance_cat_df['Feature'])
plt.xlabel('Importance Score')
plt.title('Feature Importance (zipCode One-Hot Encoded)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```


1. **Numerical vs Categorical Encoding:** There are two modelsin Python written above. For each language, the models differ by how zip code is modelled, either as a numerical variable or as a categorical variable. Given what you know about zip codes and real estate prices, how should zip code be modelled, numerically or categorically?  Is zipcode and ordinal or non-ordinal variable?

Zip codes should be modelled as **categorical variables**, specifically **non-ordinal categorical variables**. Zip codes represent discrete geographic areas with no meaningful numerical order for predicting house prices. Treating them as numerical creates meaningless splits like "zipCode > 50012.5" because zip code 50013 is not "greater than" 50012 in any way that affects property values. When encoded as categorical (via one-hot encoding), the model evaluates each neighborhood independently, capturing true location effects. **Conclusion:** Zip codes are non-ordinal categorical variables and must be encoded as such to produce accurate feature importance rankings.

2. **R vs Python Implementation Differences:** When modelling zip code as a categorical variable, the output tree and feature importance would differ quite significantly had you used R as opposed to Python. Investigate why this is the case.  What does R offer that Python does not? Which language would you say does a better job of modelling zip code as a categorical variable? Can you quote the documentation at [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html) suggesting a weakness in the Python implementation? If so, please provide a quote from the documentation.

R's `rpart` and Python's `sklearn.tree.DecisionTreeRegressor` handle categorical variables differently. **R's `rpart`** recognizes categorical variables natively and evaluates all possible category groupings (e.g., {50010, 50012} vs {50011, 50013}) to find optimal splits. No preprocessing is required.

**Python's `sklearn`** has a critical limitation. According to the official scikit-learn documentation at https://scikit-learn.org/stable/modules/tree.html:

> "The implementation of trees in scikit-learn does not support categorical variables directly. Instead, categorical variables must be encoded numerically, typically using one-hot encoding or label encoding."

This requires one-hot encoding, which transforms one categorical variable into multiple binary features (zipCode_50010, zipCode_50011, etc.). The algorithm treats each as independent, fragmenting feature importance across multiple dummy variables.

**Which is Better?** R's `rpart` handles categorical variables more effectively. It produces interpretable trees showing which neighborhoods group together, presents zip code importance as a single value, and avoids the dimensionality explosion from one-hot encoding.

3. **Are There Any Suggestions for Implementing Decision Trees in Python With Prioper Categorical Handling?** Please poke around the Internet (AI is not as helpful with new libraries) for suggestions on how to implement decision trees in Python with better (i.e. not one-hot encoding) categorical handling.  Please provide a link to the source and a quote from the source.  There is not right answer here, but please provide a thoughtful answer, I am curious to see what you find.

Modern gradient boosting libraries in Python offer native categorical support, representing the current state of the art for handling categorical variables.

**LightGBM (Microsoft)** supports categorical features natively. According to the LightGBM documentation (https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#categorical-feature-support):

> "LightGBM can use categorical features directly (without one-hot encoding). The algorithm will find the optimal split on them. Note: you should convert your categorical features to int type; it cannot use categorical values which are not preprocessed to be contiguous, non-negative integers."

**CatBoost (Yandex)** also provides native categorical support. From the CatBoost documentation (https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic):

> "CatBoost can work with categorical features without any preprocessing. The algorithm uses various statistics of combinations of categorical features and combinations of categorical and numerical features."

**Recommendation:** For datasets with many categorical variables, use LightGBM or CatBoost to avoid one-hot encoding limitations. For simple, interpretable trees, R's `rpart` remains the gold standard.

